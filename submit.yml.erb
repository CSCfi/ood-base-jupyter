# Job submission configuration file
#
---
cluster: "<%= ENV["CSC_CLUSTER"] -%>"

<% part = SlurmReservation.partition_to_use(csc_slurm_reservation, csc_slurm_partition) %>

<% max_mem_per_cpu = SlurmLimits.limits.fetch(part, {}).fetch(:max_mem_per_cpu, 0).to_f %>
<% mem = (max_mem_per_cpu * 1024 * csc_cores.to_i).to_i %>

<% n_gpus = csc_gpu.to_i %>
<% gpu_type = SlurmLimits.limits&.fetch(part, nil)&.fetch(:gpu_types)&.first %>
<% gres = [] %>
<% gres.push("nvme:#{csc_nvme}") if csc_nvme.to_i > 0 %>
<% gres.push("gpu:#{gpu_type}:#{n_gpus}") if gpu_type %>
#
# Configure the content of the job script for the batch job here
# @see http://www.rubydoc.info/gems/ood_core/OodCore/BatchConnect/Template
#
batch_connect:
  template: "basic"
  conn_params:
    - jp_type
script:
  # accounting_id: '<%# csc_slurm_account %>'
  # queue_name: '<%# csc_slurm_partition %>'
  native:
      - '-c'
      - '<%= csc_cores  %>'
      - '-t'
      - '<%= SlurmReservation.max_time(csc_slurm_reservation, csc_time) %>'
      <% if max_mem_per_cpu > 0 %>
      - '--mem=<%= mem -%>M'
      <% else %>
      - '--mem=<%= csc_memory -%>G'
      <% end %>
      <% unless gres.empty? %>
      - '--gres=<%= gres.join(",") -%>'
      <% end %>
      - '--partition=<%= part %>'
      - '--propagate=NONE'
